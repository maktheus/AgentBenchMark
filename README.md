# üìö Documenta√ß√£o Completa: Microservi√ßo de Benchmark de Agents

![AI Benchmark Service Architecture](https://i.imgur.com/8ZkRj7l.png)

## üìå Tabela de Conte√∫dos

- [1. Vis√£o Geral](#1-vis√£o-geral)
- [2. Arquitetura do Sistema](#2-arquitetura-do-sistema)
  - [2.1 Componentes Principais](#21-componentes-principais)
  - [2.2 Fluxo de Dados](#22-fluxo-de-dados)
  - [2.3 Diagrama de Componentes](#23-diagrama-de-componentes)
- [3. Documenta√ß√£o da API](#3-documenta√ß√£o-da-api)
  - [3.1 Autentica√ß√£o](#31-autentica√ß√£o)
  - [3.2 Endpoints](#32-endpoints)
  - [3.3 Exemplos de Uso](#33-exemplos-de-uso)
- [4. Configura√ß√£o e Implanta√ß√£o](#4-configura√ß√£o-e-implanta√ß√£o)
  - [4.1 Requisitos](#41-requisitos)
  - [4.2 Configura√ß√£o Local](#42-configura√ß√£o-local)
  - [4.3 Implanta√ß√£o em Produ√ß√£o](#43-implanta√ß√£o-em-produ√ß√£o)
- [5. Monitoramento e Observabilidade](#5-monitoramento-e-observabilidade)
  - [5.1 M√©tricas-Chave](#51-m√©tricas-chave)
  - [5.2 Dashboard do Grafana](#52-dashboard-do-grafana)
  - [5.3 Alertas](#53-alertas)
- [6. Testes e Valida√ß√£o](#6-testes-e-valida√ß√£o)
  - [6.1 Testes de Unidade](#61-testes-de-unidade)
  - [6.2 Testes de Carga](#62-testes-de-carga)
  - [6.3 Benchmark de Refer√™ncia](#63-benchmark-de-refer√™ncia)
- [7. Opera√ß√µes e Manuten√ß√£o](#7-opera√ß√µes-e-manuten√ß√£o)
  - [7.1 Troubleshooting](#71-troubleshooting)
  - [7.2 Atualiza√ß√µes](#72-atualiza√ß√µes)
  - [7.3 Backup e Recupera√ß√£o](#73-backup-e-recupera√ß√£o)
- [8. Extensibilidade e Contribui√ß√£o](#8-extensibilidade-e-contribui√ß√£o)
  - [8.1 Adicionando Novos Agents](#81-adicionando-novos-agents)
  - [8.2 Criando Novos Benchmarks](#82-criando-novos-benchmarks)
  - [8.3 Contribui√ß√£o para o Projeto](#83-contribui√ß√£o-para-o-projeto)
- [9. Ap√™ndices](#9-ap√™ndices)
  - [9.1 Gloss√°rio](#91-gloss√°rio)
  - [9.2 Refer√™ncias](#92-refer√™ncias)

---

## 1. Vis√£o Geral

O **Microservi√ßo de Benchmark de Agents** √© uma solu√ß√£o completa para avalia√ß√£o padronizada de agentes de intelig√™ncia artificial, incluindo modelos de linguagem grandes (LLMs), agentes aut√¥nomos e sistemas baseados em IA. O sistema permite comparar agentes em m√∫ltiplas dimens√µes de desempenho com m√©tricas objetivas e reprodut√≠veis.

### üéØ Objetivos Principais

- **Padroniza√ß√£o**: Executar benchmarks consistentes com metodologia clara
- **Comparabilidade**: Permitir compara√ß√£o justa entre diferentes agentes
- **Transpar√™ncia**: Tornar m√©tricas e metodologia acess√≠veis
- **Escalabilidade**: Suportar avalia√ß√£o de m√∫ltiplos agentes simultaneamente
- **Extensibilidade**: Facilitar a adi√ß√£o de novos agents e benchmarks

### üìä M√©tricas Avaliadas

| Categoria | M√©tricas |
|-----------|----------|
| **Precis√£o** | Exatid√£o, Taxa de Acerto, Consist√™ncia |
| **Desempenho** | Lat√™ncia, Throughput, P95/P99 |
| **Capacidades** | Racioc√≠nio, Matem√°tica, √âtica, Seguran√ßa |
| **Custo** | Uso de Tokens, Custo Estimado |
| **Qualidade** | Coer√™ncia, Hallucina√ß√£o, Alinhamento |

### üåê Casos de Uso

- **Desenvolvedores de IA**: Comparar modelos durante desenvolvimento
- **Equipes de Produto**: Selecionar agentes para integra√ß√£o em produtos
- **Pesquisadores**: Validar hip√≥teses sobre capacidades de agentes
- **Organiza√ß√µes Regulat√≥rias**: Avaliar conformidade com padr√µes

---

## 2. Arquitetura do Sistema

### 2.1 Componentes Principais

#### **API Gateway**
- **Fun√ß√£o**: Ponto √∫nico de entrada para todas as requisi√ß√µes
- **Tecnologia**: NGINX
- **Recursos**:
  - Roteamento inteligente
  - Autentica√ß√£o JWT e API Keys
  - Rate limiting configur√°vel
  - Logging detalhado

#### **Servi√ßo de Benchmark (Core)**
- **Tecnologia**: Python 3.11 + FastAPI
- **Principais Responsabilidades**:
  - Receber e validar requisi√ß√µes de benchmark
  - Orquestrar execu√ß√£o de tarefas
  - Coletar e processar resultados
  - Gerar relat√≥rios

#### **Sistema de Agents (Adapters)**
- **Padr√£o**: Adapter Pattern
- **Estrutura**:
  ```mermaid
  classDiagram
      class AgentInterface {
          <<interface>>
          + query(prompt: str, context: dict) dict
          + get_info() dict
      }
      
      class OpenAIAgentAdapter {
          - client: httpx.AsyncClient
          - config: OpenAIConfig
          + query(prompt: str, context: dict) dict
          + get_info() dict
      }
      
      class AnthropicAgentAdapter {
          - client: httpx.AsyncClient
          - config: AnthropicConfig
          + query(prompt: str, context: dict) dict
          + get_info() dict
      }
      
      AgentInterface <|.. OpenAIAgentAdapter
      AgentInterface <|.. AnthropicAgentAdapter
  ```

#### **Orquestrador de Tarefas**
- **Tecnologia**: Celery + Redis
- **Fluxo**:
  1. Recebe configura√ß√£o do benchmark
  2. Carrega dataset apropriado
  3. Distribui tarefas para agents
  4. Coleta respostas
  5. Dispara avalia√ß√£o

#### **Sistema de Avalia√ß√£o**
- **Avalia√ß√£o Autom√°tica**:
  - Compara√ß√£o com gabarito
  - M√©tricas tradicionais (BLEU, ROUGE)
  - An√°lise de tokens
- **LLM-as-a-Judge**:
  - Uso de modelo confi√°vel para avalia√ß√£o subjetiva
  - Prompting estruturado para consist√™ncia
  - Sistema de pondera√ß√£o de votos

#### **Armazenamento**
- **Resultados**: PostgreSQL (relacional)
- **Datasets**: MinIO/S3 (objeto)
- **Cache/Fila**: Redis

### 2.2 Fluxo de Dados

```mermaid
sequenceDiagram
    participant User as Usu√°rio
    participant API as API Gateway
    participant Service as Benchmark Service
    participant Orchestrator as Orchestrator
    participant Agents as Agents
    participant Evaluator as Evaluator
    participant DB as PostgreSQL
    
    User->>API: POST /benchmark/run
    API->>Service: Encaminha requisi√ß√£o
    Service->>Orchestrator: Cria run_id e enfileira
    Orchestrator->>Orchestrator: Carrega dataset
    loop Para cada tarefa
        Orchestrator->>Agents: Envia para todos agents
        par Respostas dos agents
            Agents->>Orchestrator: Respostas
            Orchestrator->>Evaluator: Envia para avalia√ß√£o
            Evaluator->>Evaluator: Processa m√©tricas
            Evaluator->>DB: Salva resultados
        end
    end
    Orchestrator->>Service: Atualiza status
    Service->>API: Retorna status
    API->>User: Resposta HTTP
```

### 2.3 Diagrama de Componentes

```mermaid
C4Context
  title Arquitetura do Microservi√ßo de Benchmark

  Person(usuario, "Usu√°rio", "Inicia e consulta benchmarks")
  System_Ext(openai, "OpenAI API", "Agentes GPT")
  System_Ext(anthropic, "Anthropic API", "Agentes Claude")
  System_Ext(custom_agents, "Agentes Personalizados", "HTTP/GRPC")

  System(benchmark_service, "Microservi√ßo de Benchmark", "Core do sistema")
  SystemDb(postgres, "PostgreSQL", "Resultados e configura√ß√µes")
  SystemCache(redis, "Redis", "Cache e fila de tarefas")
  SystemDb(minio, "MinIO/S3", "Datasets e relat√≥rios")

  Rel(usuario, benchmark_service, "HTTP (API REST)")
  Rel(benchmark_service, openai, "API Key (HTTPS)")
  Rel(benchmark_service, anthropic, "API Key (HTTPS)")
  Rel(benchmark_service, custom_agents, "HTTP/GRPC")
  Rel(benchmark_service, postgres, "SQL (JDBC)")
  Rel(benchmark_service, redis, "Celery Tasks")
  Rel(benchmark_service, minio, "S3 API")

  Boundary(infra, "Infraestrutura") {
    System_Ext(grafana, "Grafana", "Monitoramento")
    System_Ext(prometheus, "Prometheus", "M√©tricas")
    System_Ext(jaeger, "Jaeger", "Tracing Distribu√≠do")
  }

  Rel(benchmark_service, grafana, "Metrics (Prometheus)")
  Rel(benchmark_service, jaeger, "Traces (OpenTelemetry)")
```

---

## 3. Documenta√ß√£o da API

### 3.1 Autentica√ß√£o

Todas as requisi√ß√µes requerem autentica√ß√£o via token Bearer:

```http
GET /benchmark/list HTTP/1.1
Host: api.benchmark.example.com
Authorization: Bearer <SEU_API_KEY>
```

- **Obten√ß√£o de API Key**: Via painel administrativo ou processo de onboarding
- **Escopo de Permiss√µes**:
  - `benchmark:read`: Consultar benchmarks
  - `benchmark:write`: Criar novos benchmarks
  - `results:read`: Acessar resultados detalhados

### 3.2 Endpoints

#### `POST /benchmark/run` - Iniciar Novo Benchmark

**Descri√ß√£o**: Submete uma nova execu√ß√£o de benchmark para processamento ass√≠ncrono

**Par√¢metros**:

| Campo | Tipo | Obrigat√≥rio | Descri√ß√£o | Exemplo |
|-------|------|-------------|-----------|---------|
| agents | array | Sim | Lista de agents a serem avaliados | `["gpt-4", "claude-3"]` |
| benchmark | string | Sim | ID do benchmark a ser executado | `"mmlu-reasoning-v1"` |
| config | object | N√£o | Configura√ß√µes espec√≠ficas do benchmark | `{"temperature": 0.7}` |

**Exemplo de Requisi√ß√£o**:
```json
{
  "agents": ["gpt-4-turbo", "claude-3-opus"],
  "benchmark": "mmlu-reasoning-v1",
  "config": {
    "temperature": 0.7,
    "max_tokens": 1024
  }
}
```

**Resposta de Sucesso (202 Accepted)**:
```json
{
  "run_id": "bf9d8e7a-1b2c-4d3e-8f7a-6b5c4d3e2f1a",
  "status": "queued",
  "created_at": "2024-05-15T14:30:00Z"
}
```

#### `GET /benchmark/{id}` - Consultar Status

**Descri√ß√£o**: Obt√©m o status atual de uma execu√ß√£o de benchmark

**Par√¢metros**:
- `id` (path): ID da execu√ß√£o (UUID)

**Resposta**:
```json
{
  "run_id": "bf9d8e7a-1b2c-4d3e-8f7a-6b5c4d3e2f1a",
  "status": "completed",
  "progress": 1.0,
  "started_at": "2024-05-15T14:30:00Z",
  "completed_at": "2024-05-15T14:32:27Z",
  "results_url": "/results/bf9d8e7a-1b2c-4d3e-8f7a-6b5c4d3e2f1a"
}
```

#### `GET /results/{id}` - Obter Resultados

**Descri√ß√£o**: Retorna resultados detalhados de um benchmark conclu√≠do

**Par√¢metros**:
- `id` (path): ID da execu√ß√£o (UUID)

**Resposta**:
```json
{
  "run_id": "bf9d8e7a-1b2c-4d3e-8f7a-6b5c4d3e2f1a",
  "benchmark": "mmlu-reasoning-v1",
  "agents": [
    {
      "id": "gpt-4-turbo",
      "metrics": {
        "accuracy": 87.3,
        "latency_avg": 4.2,
        "tokens_avg": 1428,
        "consistency": 4.7
      },
      "category_scores": {
        "mathematics": 92.4,
        "logical_reasoning": 88.2
      }
    }
  ],
  "summary": {
    "top_performer": "gpt-4-turbo",
    "critical_observations": [
      "Melhor desempenho em racioc√≠nio matem√°tico",
      "Consist√™ncia superior em m√∫ltiplas categorias"
    ]
  }
}
```

#### `GET /benchmark/list` - Listar Benchmarks Dispon√≠veis

**Descri√ß√£o**: Retorna lista de benchmarks dispon√≠veis para execu√ß√£o

**Resposta**:
```json
[
  {
    "id": "mmlu-reasoning-v1",
    "name": "MMLU Reasoning Benchmark v1",
    "description": "Avalia√ß√£o de racioc√≠nio l√≥gico baseada no MMLU",
    "categories": ["mathematics", "formal_logic", "symbolic_reasoning"],
    "question_count": 150
  },
  {
    "id": "gsm8k-math-v2",
    "name": "GSM8K Math Benchmark v2",
    "description": "Problemas matem√°ticos de escola prim√°ria",
    "categories": ["arithmetic", "algebra"],
    "question_count": 850
  }
]
```

### 3.3 Exemplos de Uso

#### Executando um Benchmark B√°sico

```bash
curl -X POST https://api.benchmark.example.com/benchmark/run \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "agents": ["gpt-4-turbo", "claude-3-opus"],
    "benchmark": "mmlu-reasoning-v1"
  }'
```

#### Monitorando o Progresso

```bash
curl -X GET https://api.benchmark.example.com/benchmark/bf9d8e7a-1b2c-4d3e-8f7a-6b5c4d3e2f1a \
  -H "Authorization: Bearer YOUR_API_KEY"
```

#### Obtendo Resultados Formatados

```bash
# Resultado JSON
curl -X GET https://api.benchmark.example.com/results/bf9d8e7a-1b2c-4d3e-8f7a-6b5c4d3e2f1a \
  -H "Authorization: Bearer YOUR_API_KEY"

# Relat√≥rio PDF
curl -X GET https://api.benchmark.example.com/results/bf9d8e7a-1b2c-4d3e-8f7a-6b5c4d3e2f1a/report \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -o benchmark_report.pdf
```

---

## 4. Configura√ß√£o e Implanta√ß√£o

### 4.1 Requisitos

#### Ambiente de Desenvolvimento
- Python 3.11+
- Docker 20.10+
- Docker Compose 2.10+
- PostgreSQL 15+
- Redis 7+

#### Ambiente de Produ√ß√£o
- Kubernetes 1.25+
- PostgreSQL RDS ou cluster gerenciado
- Redis ElastiCache ou cluster gerenciado
- S3/MinIO para armazenamento de objetos
- Prometheus/Grafana para monitoramento

### 4.2 Configura√ß√£o Local

#### Passo 1: Clonar o reposit√≥rio
```bash
git clone https://github.com/your-org/benchmark-service.git
cd benchmark-service
```

#### Passo 2: Configurar vari√°veis de ambiente
Crie um arquivo `.env` na raiz do projeto:
```env
# Banco de Dados
POSTGRES_URL=postgresql://user:pass@postgres:5432/benchmark
REDIS_URL=redis://redis:6379/0

# Configura√ß√£o de Agents
OPENAI_API_KEY=sk-xxx
ANTHROPIC_API_KEY=xxx

# Workers
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/0

# Seguran√ßa
SECRET_KEY=your_strong_secret_here
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Armazenamento
MINIO_ENDPOINT=minio:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
```

#### Passo 3: Iniciar servi√ßos com Docker Compose
```bash
docker-compose up --build -d
```

#### Passo 4: Executar migra√ß√µes do banco
```bash
docker-compose exec web alembic upgrade head
```

#### Passo 5: Acessar a documenta√ß√£o da API
Abra em seu navegador:
```
http://localhost:8000/docs
```

### 4.3 Implanta√ß√£o em Produ√ß√£o

#### Arquitetura de Produ√ß√£o Recomendada

![Production Architecture](https://i.imgur.com/5GjKZQl.png)

#### Passo 1: Configurar Secrets no Kubernetes
```bash
kubectl create secret generic benchmark-secrets \
  --from-literal=OPENAI_API_KEY=your_key \
  --from-literal=ANTHROPIC_API_KEY=your_key \
  --from-literal=SECRET_KEY=strong_secret
```

#### Passo 2: Aplicar manifestos Kubernetes
```bash
kubectl apply -f k8s/
```

#### Passo 3: Configurar Ingress
```yaml
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: benchmark-ingress
  annotations:
    kubernetes.io/ingress.class: "nginx"
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
  - host: api.benchmark.example.com
    http:
      paths:
      - path: /(.*)
        pathType: Prefix
        backend:
          service:
            name: benchmark-service
            port:
              number: 80
```

#### Passo 4: Configurar Horizontal Pod Autoscaler
```bash
kubectl apply -f k8s/horizontal-pod-autoscaler.yaml
```

#### Estrat√©gia de Release
- **Canary Releases**: 5% de tr√°fego inicial, aumentando gradualmente
- **Health Checks**: `/health` para liveness, `/ready` para readiness
- **Rollback Autom√°tico**: Se taxa de erro > 1% ou lat√™ncia P95 > 5s

---

## 5. Monitoramento e Observabilidade

### 5.1 M√©tricas-Chave

#### M√©tricas de Disponibilidade
| M√©trica | Limite Aceit√°vel | A√ß√£o |
|---------|------------------|------|
| Taxa de Sucesso | > 99.5% | Investigar erros 5xx |
| Tempo M√©dio de Resposta | < 1s | Otimizar agents lentos |
| Erros 5xx | < 0.5% | Verificar logs de erro |

#### M√©tricas de Desempenho
| M√©trica | Limite Aceit√°vel | A√ß√£o |
|---------|------------------|------|
| Lat√™ncia P95 | < 2s | Ajustar timeout |
| Throughput | > 50 RPS | Escalar workers |
| Taxa de Erro | < 0.1% | Investigar causa |

#### M√©tricas de Agentes
| M√©trica | Descri√ß√£o |
|---------|-----------|
| Precis√£o por Categoria | % de respostas corretas por dom√≠nio |
| Lat√™ncia por Agent | Tempo m√©dio de resposta |
| Uso de Tokens | Tokens consumidos por tarefa |
| Consist√™ncia | Varia√ß√£o nas respostas para inputs similares |

### 5.2 Dashboard do Grafana

![Grafana Dashboard](https://i.imgur.com/6XfYz9l.png)

**Configura√ß√£o Completa**:
- [Download do JSON do Dashboard](grafana/dashboards/benchmark-dashboard.json)
- [Configura√ß√£o de Provisionamento](grafana/provisioning/)

**Principais Pain√©is**:
1. **Taxa de Solicita√ß√£o por Status**: Monitora sucesso/erros
2. **Lat√™ncia Percentis**: P50, P95, P99
3. **Uso de Tokens por Agent**: Compara√ß√£o de custo
4. **Throughput de Tarefas**: Tarefas iniciadas vs conclu√≠das
5. **Taxa de Sucesso**: Vis√£o geral da sa√∫de do servi√ßo

### 5.3 Alertas

#### Alertas Cr√≠ticos (PagerDuty/Slack)
| Alerta | Condi√ß√£o | A√ß√£o |
|--------|----------|------|
| **Alta Taxa de Erro** | `rate(http_request_duration_seconds_count{status_code!~"2.."}[5m]) > 0.01` | Investigar imediatamente |
| **Lat√™ncia Elevada** | `histogram_quantile(0.95, rate(...)) > 5` | Escalar workers |
| **Baixo Throughput** | `rate(benchmark_task_completed_total[5m]) < 10` | Verificar agents |
| **Falha no Banco** | `up{job="postgres"} == 0` | Contato com DBA |

#### Alertas de Aviso (Email)
| Alerta | Condi√ß√£o |
|--------|----------|
| Uso de Tokens An√¥malo | `increase(agent_token_usage_total[1h]) > 2 * avg_over_time(...[24h])` |
| Baixa Utiliza√ß√£o de Recursos | `avg(rate(container_cpu_usage_seconds_total[5m])) < 0.3` |
| Tarefas Estagnadas | `benchmark_task_started_total - benchmark_task_completed_total > 100` |

---

## 6. Testes e Valida√ß√£o

### 6.1 Testes de Unidade

#### Estrutura de Testes
```
tests/
‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ test_agents.py
‚îÇ   ‚îú‚îÄ‚îÄ test_benchmark.py
‚îÇ   ‚îî‚îÄ‚îÄ test_evaluation.py
‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îú‚îÄ‚îÄ test_api.py
‚îÇ   ‚îî‚îÄ‚îÄ test_dataset_loader.py
‚îî‚îÄ‚îÄ e2e/
    ‚îî‚îÄ‚îÄ test_full_workflow.py
```

#### Exemplo de Teste de Unidade
```python
# tests/unit/test_agents.py
import pytest
from agents.openai_adapter import OpenAIAgentAdapter, OpenAIConfig

@pytest.mark.asyncio
async def test_openai_adapter_success(mocker):
    """Testa resposta bem-sucedida do adapter OpenAI"""
    # Configura mock
    mock_response = mocker.Mock()
    mock_response.status_code = 200
    mock_response.json.return_value = {
        "choices": [{"message": {"content": "Test response"}}],
        "usage": {"total_tokens": 50}
    }
    mock_response.elapsed.total_seconds.return_value = 0.5
    
    mocker.patch("httpx.AsyncClient.post", return_value=mock_response)
    
    # Configura adapter
    config = OpenAIConfig(api_key="test-key")
    adapter = OpenAIAgentAdapter(config)
    
    # Executa teste
    result = await adapter.query("Hello", {"context": "test"})
    
    # Verifica resultados
    assert result["response"] == "Test response"
    assert result["usage"]["total_tokens"] == 50
    assert 0.4 <= result["latency"] <= 0.6
```

#### Cobertura de Testes
- **M√≠nimo Aceit√°vel**: 80% de cobertura
- **Componentes Cr√≠ticos**: 95%+ (sistema de avalia√ß√£o, orquestra√ß√£o)
- **Relat√≥rio Autom√°tico**: Integrado ao pipeline CI/CD

### 6.2 Testes de Carga

#### Configura√ß√£o do Locust
```python
# locustfile.py
class BenchmarkUser(HttpUser):
    wait_time = between(1, 5)
    
    @task(7)
    def submit_benchmark(self):
        payload = {
            "agents": random.sample(AGENT_LIST, k=random.randint(1, 3)),
            "benchmark": random.choice(BENCHMARK_TYPES),
            "config": generate_random_config()
        }
        
        with self.client.post(
            "/benchmark/run", 
            json=payload,
            headers=self.headers,
            catch_response=True
        ) as response:
            if response.status_code != 202:
                response.failure(f"Unexpected status code: {response.status_code}")
```

#### Resultados Esperados para 100 RPS
```
Type     Name                                                # reqs      # fails  |     Avg     Min     Max  Median  |   req/s failures/s
--------|---------------------------------------------------|-------|-----------|--------|-------|-------|--------|-------|-----------
POST     /benchmark/run                                      14,235     0(0.00%)  |    412     123    2487     389  |   47.50    0.00
GET      /benchmark/{id}                                      4,028     0(0.00%)  |     98      45     512      95  |   13.45    0.00
GET      /benchmark/list                                      2,015     0(0.00%)  |     76      32     387      72  |    6.73    0.00
--------|---------------------------------------------------|-------|-----------|--------|-------|-------|--------|-------|-----------
         Aggregated                                          20,278     0(0.00%)  |    312     32    2487     287  |   67.68    0.00
```

#### Crit√©rios de Aceita√ß√£o
- **Throughput**: > 80 RPS sustentado por 15 minutos
- **Lat√™ncia P95**: < 2 segundos para submiss√£o
- **Taxa de Erro**: < 0.1%
- **Estabilidade**: Nenhuma queda durante o teste

### 6.3 Benchmark de Refer√™ncia

#### Protocolo de Benchmark
1. **Prepara√ß√£o**:
   - Limpar cache
   - Reiniciar workers
   - Carregar dataset pr√©-definido

2. **Execu√ß√£o**:
   - Executar 3 vezes para cada agent
   - Usar seeds fixos para reprodutibilidade
   - Registrar m√©tricas detalhadas

3. **Avalia√ß√£o**:
   - Comparar com resultados de refer√™ncia
   - Analisar varia√ß√µes significativas
   - Gerar relat√≥rio detalhado

#### Dataset de Refer√™ncia (MMLU Reasoning)
```json
{
  "benchmark_id": "mmlu-reasoning-v1",
  "description": "MMLU subset for logical reasoning evaluation",
  "categories": ["mathematics", "formal_logic", "symbolic_reasoning"],
  "total_questions": 150,
  "data": [
    {
      "id": "math-001",
      "category": "mathematics",
      "question": "If a car travels at 60 km/h for 2.5 hours...",
      "options": ["A) 45 km/h", "B) 48 km/h", "C) 52 km/h", "D) 55 km/h"],
      "answer": "C",
      "rationale": "Total distance = (60*2.5) + (40*1.5) = 210 km..."
    }
  ]
}
```

#### Resultados de Refer√™ncia Esperados
| Agent | Precis√£o Geral | Matem√°tica | L√≥gica Formal | Racioc√≠nio Simb√≥lico | Lat√™ncia M√©dia |
|-------|----------------|------------|---------------|----------------------|----------------|
| GPT-4 | 87.3% | 92.4% | 88.2% | 82.0% | 4.2s |
| Claude-3 | 82.1% | 85.6% | 84.0% | 80.4% | 6.8s |
| Llama-3 | 78.5% | 82.3% | 76.8% | 75.2% | 5.1s |

---

## 7. Opera√ß√µes e Manuten√ß√£o

### 7.1 Troubleshooting

#### Problema Comum #1: Erros 500 ao Submeter Benchmarks
- **Sintomas**:
  - Respostas HTTP 500
  - Logs mostram `Database connection timeout`
  
- **Causa Prov√°vel**:
  - Conex√µes com o PostgreSQL esgotadas
  
- **Solu√ß√£o**:
  ```bash
  # Verificar conex√µes ativas
  kubectl exec -it <postgres-pod> -- psql -c "SELECT * FROM pg_stat_activity;"
  
  # Ajustar pool de conex√µes
  # No arquivo de configura√ß√£o do servi√ßo:
  DATABASE_POOL_SIZE=20
  DATABASE_MAX_OVERFLOW=10
  ```

#### Problema Comum #2: Agents N√£o Respondem
- **Sintomas**:
  - Status "running" por muito tempo
  - Logs mostram timeouts com APIs de agents
  
- **Causa Prov√°vel**:
  - Limite de taxa excedido nas APIs de agents
  - Configura√ß√£o incorreta de chaves API
  
- **Solu√ß√£o**:
  ```bash
  # Verificar m√©tricas de agents
  kubectl exec -it <grafana-pod> -- grafana-cli plugins ls
  
  # Ajustar configura√ß√£o de rate limiting
  AGENT_RATE_LIMIT_OPENAI=60  # 60 requisi√ß√µes/minuto
  AGENT_RATE_LIMIT_ANTHROPIC=45
  ```

#### Fluxo de Diagn√≥stico
```mermaid
graph TD
    A[Problema Reportado] --> B{Erro no Cliente?}
    B -->|Sim| C[Verificar Requisi√ß√£o]
    B -->|N√£o| D{Erro no Servi√ßo?}
    D -->|Sim| E[Verificar Logs do Servi√ßo]
    D -->|N√£o| F{Erro nos Agents?}
    F -->|Sim| G[Verificar Conex√£o com Agents]
    F -->|N√£o| H{Erro no Banco?}
    H -->|Sim| I[Verificar Conex√µes com Banco]
    H -->|N√£o| J[Verificar Infraestrutura]
```

### 7.2 Atualiza√ß√µes

#### Ciclo de Release
- **Patch Releases** (semanal): Corre√ß√µes de bugs cr√≠ticos
- **Minor Releases** (mensal): Novos features n√£o quebr√°veis
- **Major Releases** (trimestral): Mudan√ßas significativas

#### Checklist de Release
1. [ ] Testes de unidade e integra√ß√£o passando
2. [ ] Teste de carga com resultados aceit√°veis
3. [ ] Documenta√ß√£o atualizada
4. [ ] Schema de banco migrado com sucesso
5. [ ] Canary release com 5% de tr√°fego
6. [ ] Rollout completo ap√≥s 1 hora sem problemas

#### Rollback Procedure
1. Reverter para a vers√£o anterior no Kubernetes:
   ```bash
   kubectl rollout undo deployment/benchmark-service
   ```
2. Verificar sa√∫de do sistema:
   ```bash
   kubectl get pods -l app=benchmark
   curl http://api.benchmark.example.com/health
   ```
3. Monitorar m√©tricas cr√≠ticas por 30 minutos

### 7.3 Backup e Recupera√ß√£o

#### Estrat√©gia de Backup
- **Frequ√™ncia**:
  - Resultados: Backup a cada 4 horas
  - Configura√ß√µes: Backup di√°rio
  - Datasets: Backup semanal

- **Localiza√ß√£o**:
  - Regi√£o prim√°ria: AWS us-east-1
  - Regi√£o secund√°ria: AWS eu-west-1

#### Procedimento de Recupera√ß√£o
1. Restaurar banco de dados:
   ```bash
   pg_restore -h <DB_HOST> -U <USER> -d <DB_NAME> backup.dump
   ```
2. Restaurar datasets:
   ```bash
   aws s3 cp s3://backup-bucket/datasets/ ./datasets --recursive
   ```
3. Validar integridade:
   ```bash
   python validate_backups.py --dataset-path ./datasets
   ```

#### RTO/RPO
- **RTO (Recovery Time Objective)**: < 30 minutos
- **RPO (Recovery Point Objective)**: < 4 horas

---

## 8. Extensibilidade e Contribui√ß√£o

### 8.1 Adicionando Novos Agents

#### Passo 1: Criar Adapter
```python
# agents/my_new_agent.py
from typing import Dict, Any
from .base import AgentInterface

class MyNewAgentConfig:
    api_key: str
    endpoint: str = "https://api.myagent.com/v1"
    timeout: float = 30.0

class MyNewAgentAdapter(AgentInterface):
    def __init__(self, config: MyNewAgentConfig):
        self.config = config
        # Inicializa√ß√£o do cliente
    
    async def query(self, prompt: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        # Implementa√ß√£o da chamada √† API
        pass
    
    def get_info(self) -> Dict[str, Any]:
        return {
            "name": "my-new-agent",
            "version": "1.0.0",
            "capabilities": ["text-generation", "reasoning"]
        }
```

#### Passo 2: Registrar no Sistema
```python
# agents/__init__.py
from .my_new_agent import MyNewAgentAdapter

# Mapeamento de nomes para adapters
AGENT_ADAPTERS = {
    "my-new-agent": MyNewAgentAdapter,
    # outros agents...
}
```

#### Passo 3: Adicionar Testes
```python
# tests/unit/test_my_new_agent.py
import pytest
from agents.my_new_agent import MyNewAgentAdapter, MyNewAgentConfig

@pytest.mark.asyncio
async def test_my_new_agent():
    config = MyNewAgentConfig(api_key="test")
    adapter = MyNewAgentAdapter(config)
    result = await adapter.query("Hello")
    assert "response" in result
```

### 8.2 Criando Novos Benchmarks

#### Estrutura de Dataset
```json
{
  "benchmark_id": "custom-benchmark-v1",
  "description": "Custom benchmark for specific use case",
  "categories": ["category1", "category2"],
  "metadata": {
    "source": "internal",
    "total_questions": 100,
    "difficulty": "medium"
  },
  "data": [
    {
      "id": "q001",
      "category": "category1",
      "question": "What is 2+2?",
      "options": ["A) 3", "B) 4", "C) 5", "D) 6"],
      "answer": "B",
      "rationale": "Basic arithmetic: 2+2=4",
      "expected_metrics": {
        "min_reasoning_steps": 1,
        "expected_time": 5.0
      }
    }
  ]
}
```

#### Processo de Adi√ß√£o
1. Criar arquivo JSON no formato acima
2. Armazenar em `datasets/custom-benchmark-v1.json`
3. Registrar no sistema:
   ```python
   # services/dataset_loader.py
   BENCHMARK_DATASETS = {
       "custom-benchmark-v1": "datasets/custom-benchmark-v1.json",
       # outros benchmarks...
   }
   ```
4. Adicionar testes de valida√ß√£o

### 8.3 Contribui√ß√£o para o Projeto

#### Diretrizes de Contribui√ß√£o
1. **Fork do reposit√≥rio**
2. **Criar branch feature**:
   ```bash
   git checkout -b feature/my-feature
   ```
3. **Implementar mudan√ßas** com testes
4. **Atualizar documenta√ß√£o**
5. **Submeter Pull Request** com descri√ß√£o detalhada

#### Checklist de Pull Request
- [ ] Testes cobrindo novas funcionalidades
- [ ] Documenta√ß√£o atualizada
- [ ] Nenhuma quebra de compatibilidade
- [ ] Mensagens de commit claras e descritivas
- [ ] Formata√ß√£o do c√≥digo seguindo padr√µes

#### Processo de Revis√£o
1. An√°lise t√©cnica pelo mantenedor
2. Testes de integra√ß√£o autom√°tica
3. Discuss√£o de poss√≠veis melhorias
4. Aprova√ß√£o por 2 revisores
5. Merge ap√≥s CI/CD passar

---

## 9. Ap√™ndices

### 9.1 Gloss√°rio

| Termo | Defini√ß√£o |
|-------|-----------|
| **Agent** | Modelo de IA ou sistema avaliado no benchmark |
| **Benchmark** | Conjunto padronizado de tarefas para avalia√ß√£o |
| **Adapter** | Componente que padroniza interface com agents |
| **Orquestrador** | Componente que gerencia execu√ß√£o de benchmarks |
| **LLM-as-a-Judge** | Uso de modelo de linguagem para avalia√ß√£o subjetiva |
| **Throughput** | N√∫mero de opera√ß√µes conclu√≠das por unidade de tempo |
| **P95 Latency** | Lat√™ncia m√°xima para 95% das requisi√ß√µes |

### 9.2 Refer√™ncias

#### Benchmarks Padr√£o
- **MMLU**: Massive Multitask Language Understanding
- **GSM8K**: Grade School Math 8K
- **BigBench**: Beyond the Imitation Game Benchmark
- **TruthfulQA**: Measuring Truthfulness in Language Models

#### Frameworks Relacionados
- **LangChain**: Framework para aplica√ß√µes com LLMs
- **LlamaIndex**: Ferramenta para recupera√ß√£o de informa√ß√µes
- **Weights & Biases**: Plataforma de experimenta√ß√£o de ML

#### Recursos Adicionais
- [Documenta√ß√£o OpenAPI](https://spec.openapis.org/oas/v3.0.3)
- [Guia de Boas Pr√°ticas de Microservi√ßos](https://microservices.io)
- [Padr√µes de Projeto para Sistemas Distribu√≠dos](https://docs.microsoft.com/en-us/azure/architecture/patterns/)

---

## üì¨ Suporte e Contato

Para suporte t√©cnico ou perguntas sobre o projeto:

- **Email**: support@benchmark.example.com
- **Issues no GitHub**: [github.com/your-org/benchmark-service/issues](https://github.com/your-org/benchmark-service/issues)
- **Documenta√ß√£o Completa**: [docs.benchmark.example.com](https://docs.benchmark.example.com)

---

¬© 2024 AI Benchmark Service. Todos os direitos reservados.  
Esta documenta√ß√£o √© parte do projeto open source AI Benchmark Service.  
Contribui√ß√µes s√£o bem-vindas via GitHub.